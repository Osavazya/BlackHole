name: EKS • start (create/ensure)

on:
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

# Пробрасываем значения в env, чтобы их можно было использовать в if: (env.*)
env:
  AWS_REGION:             ${{ secrets.AWS_REGION }}
  AWS_ACCOUNT_ID:         ${{ secrets.AWS_ACCOUNT_ID }}
  EKS_CLUSTER:            ${{ secrets.EKS_CLUSTER }}
  ROLE_TO_ASSUME:         ${{ secrets.ROLE_TO_ASSUME }}
  AWS_ACCESS_KEY_ID:      ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY:  ${{ secrets.AWS_SECRET_ACCESS_KEY }}

  # Параметры EKS (можно править без изменения скриптов)
  EKS_VERSION:   "1.30"
  NODE_TYPES:    '["t3.medium","t3a.medium"]'
  NODES_DESIRED: "2"
  NODES_MIN:     "1"
  NODES_MAX:     "3"

jobs:
  start:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Предпочитаем OIDC; если ROLE_TO_ASSUME пустая — падаем на AK/SK
      - name: Configure AWS (OIDC)
        if: ${{ env.ROLE_TO_ASSUME != '' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume:    ${{ env.ROLE_TO_ASSUME }}
          role-session-name: gha-eks-start
          aws-region:        ${{ env.AWS_REGION }}

      - name: Configure AWS (access keys fallback)
        if: ${{ env.ROLE_TO_ASSUME == '' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Install kubectl, eksctl, helm
        shell: bash
        run: |
          set -euo pipefail
          # kubectl
          curl -sSL -o kubectl "https://storage.googleapis.com/kubernetes-release/release/$(curl -sSL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          sudo install -m 0755 kubectl /usr/local/bin/kubectl
          # eksctl
          curl -sSL "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" \
            | sudo tar xz -C /usr/local/bin
          # helm
          sudo snap install helm --classic

      - name: Create EKS cluster if missing (eksctl)
        shell: bash
        run: |
          set -euo pipefail
          if aws eks describe-cluster --name "$EKS_CLUSTER" --region "$AWS_REGION" >/dev/null 2>&1; then
            echo "Cluster '$EKS_CLUSTER' already exists — skip create."
          else
            # Генерим eksctl.yaml (heredoc ВНУТРИ run: |, кавычки у EOF — чтобы не подставлялись переменные на этапе cat)
            cat > eksctl.yaml <<'EOF'
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: ${EKS_CLUSTER}
  region: ${AWS_REGION}
  version: "${EKS_VERSION}"
iam:
  withOIDC: true
managedNodeGroups:
  - name: ng-spot
    instanceTypes: ${NODE_TYPES}
    desiredCapacity: ${NODES_DESIRED}
    minSize: ${NODES_MIN}
    maxSize: ${NODES_MAX}
    spot: true
vpc:
  nat:
    gateway: Disable
  clusterEndpoints:
    publicAccess: true
    privateAccess: false
cloudWatch:
  clusterLogging:
    enableTypes: ["api","authenticator","controllerManager","scheduler"]
EOF
            # Подставляем реальные значения окружения
            sed -i \
              -e "s|\${EKS_CLUSTER}|${EKS_CLUSTER}|g" \
              -e "s|\${AWS_REGION}|${AWS_REGION}|g" \
              -e "s|\${EKS_VERSION}|${EKS_VERSION}|g" \
              -e "s|\${NODE_TYPES}|${NODE_TYPES}|g" \
              -e "s|\${NODES_DESIRED}|${NODES_DESIRED}|g" \
              -e "s|\${NODES_MIN}|${NODES_MIN}|g" \
              -e "s|\${NODES_MAX}|${NODES_MAX}|g" \
              eksctl.yaml

            eksctl create cluster -f eksctl.yaml --timeout=40m
          fi

      - name: Update kubeconfig
        shell: bash
        run: aws eks update-kubeconfig --region "$AWS_REGION" --name "$EKS_CLUSTER"

      # -------- AWS Load Balancer Controller --------
      - name: Ensure IAM policy for ALB controller
        shell: bash
        run: |
          set -euo pipefail
          POLICY_NAME="AWSLoadBalancerControllerIAMPolicy"
          POLICY_ARN="arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${POLICY_NAME}"
          if ! aws iam get-policy --policy-arn "$POLICY_ARN" >/dev/null 2>&1; then
            curl -sSL -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.1/docs/install/iam_policy.json
            aws iam create-policy --policy-name "$POLICY_NAME" --policy-document file://iam_policy.json >/dev/null
          fi
          echo "POLICY_ARN=$POLICY_ARN" >> "$GITHUB_ENV"

      - name: Create IAM service account for ALB controller
        shell: bash
        run: |
          set -euo pipefail
          eksctl utils associate-iam-oidc-provider --region "$AWS_REGION" --cluster "$EKS_CLUSTER" --approve
          eksctl create iamserviceaccount \
            --cluster "$EKS_CLUSTER" \
            --namespace kube-system \
            --name aws-load-balancer-controller \
            --role-name AmazonEKSLoadBalancerControllerRole \
            --attach-policy-arn "$POLICY_ARN" \
            --approve --override-existing-serviceaccounts

      - name: Install ALB controller (Helm)
        shell: bash
        run: |
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName="$EKS_CLUSTER" \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller

      - name: Show nodes & ingresses
        shell: bash
        run: |
          kubectl get nodes -o wide || true
          kubectl get ingress -A -o wide || true
