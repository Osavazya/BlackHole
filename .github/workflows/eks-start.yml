name: EKS • start (create/ensure)

on:
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION:             ${{ secrets.AWS_REGION }}
  AWS_ACCOUNT_ID:         ${{ secrets.AWS_ACCOUNT_ID }}
  EKS_CLUSTER:            ${{ secrets.EKS_CLUSTER }}
  ROLE_TO_ASSUME:         ${{ secrets.ROLE_TO_ASSUME }}
  AWS_ACCESS_KEY_ID:      ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY:  ${{ secrets.AWS_SECRET_ACCESS_KEY }}

  # Параметры кластера
  EKS_VERSION:   "1.30"
  NODE_TYPES:    '["t3.medium","t3a.medium"]'
  NODES_DESIRED: "2"
  NODES_MIN:     "1"
  NODES_MAX:     "3"

jobs:
  start:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Предпочитаем OIDC; если роль не задана — падение на AK/SK
      - name: Configure AWS (OIDC)
        if: ${{ env.ROLE_TO_ASSUME != '' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume:    ${{ env.ROLE_TO_ASSUME }}
          role-session-name: gha-eks-start
          aws-region:        ${{ env.AWS_REGION }}

      - name: Configure AWS (access keys fallback)
        if: ${{ env.ROLE_TO_ASSUME == '' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Install kubectl, eksctl, helm
        shell: bash
        run: |
          set -euo pipefail
          curl -sSL -o kubectl "https://storage.googleapis.com/kubernetes-release/release/$(curl -sSL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          sudo install -m 0755 kubectl /usr/local/bin/kubectl
          curl -sSL "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | sudo tar xz -C /usr/local/bin
          # helm
          curl -sSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          # envsubst (на runner обычно уже есть, но на всякий случай)
          sudo apt-get update -y && sudo apt-get install -y gettext-base

      - name: Create EKS cluster if missing (eksctl)
        shell: bash
        run: |
          set -euo pipefail
          if aws eks describe-cluster --name "$EKS_CLUSTER" --region "$AWS_REGION" >/dev/null 2>&1; then
            echo "Cluster '$EKS_CLUSTER' already exists — skip create."
          else
            # Шаблон с безопасным отступом; затем его срежем и подставим env
            cat > eksctl.tmpl <<'EOF'
              apiVersion: eksctl.io/v1alpha5
              kind: ClusterConfig
              metadata:
                name: ${EKS_CLUSTER}
                region: ${AWS_REGION}
                version: "${EKS_VERSION}"
              iam:
                withOIDC: true
              managedNodeGroups:
                - name: ng-spot
                  instanceTypes: ${NODE_TYPES}
                  desiredCapacity: ${NODES_DESIRED}
                  minSize: ${NODES_MIN}
                  maxSize: ${NODES_MAX}
                  spot: true
              vpc:
                nat:
                  gateway: Disable
                clusterEndpoints:
                  publicAccess: true
                  privateAccess: false
              cloudWatch:
                clusterLogging:
                  enableTypes: ["api","authenticator","controllerManager","scheduler"]
EOF
            # срезаем ведущие 2 пробела у каждой строки
            sed -i 's/^  //' eksctl.tmpl
            # подставляем значения окружения
            envsubst < eksctl.tmpl > eksctl.yaml
            echo "eksctl.yaml:"
            cat eksctl.yaml

            eksctl create cluster -f eksctl.yaml --timeout=40m
          fi

      - name: Update kubeconfig
        shell: bash
        run: aws eks update-kubeconfig --region "$AWS_REGION" --name "$EKS_CLUSTER"

      # -------- AWS Load Balancer Controller --------
      - name: Ensure IAM policy for ALB controller
        shell: bash
        run: |
          set -euo pipefail
          POLICY_NAME="AWSLoadBalancerControllerIAMPolicy"
          POLICY_ARN="arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${POLICY_NAME}"
          if ! aws iam get-policy --policy-arn "$POLICY_ARN" >/dev/null 2>&1; then
            curl -sSL -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.1/docs/install/iam_policy.json
            aws iam create-policy --policy-name "$POLICY_NAME" --policy-document file://iam_policy.json >/dev/null
          fi
          echo "POLICY_ARN=$POLICY_ARN" >> "$GITHUB_ENV"

      - name: Create IAM service account for ALB controller
        shell: bash
        run: |
          set -euo pipefail
          eksctl utils associate-iam-oidc-provider --region "$AWS_REGION" --cluster "$EKS_CLUSTER" --approve
          eksctl create iamserviceaccount \
            --cluster "$EKS_CLUSTER" \
            --namespace kube-system \
            --name aws-load-balancer-controller \
            --role-name AmazonEKSLoadBalancerControllerRole \
            --attach-policy-arn "$POLICY_ARN" \
            --approve --override-existing-serviceaccounts

      - name: Install ALB controller (Helm)
        shell: bash
        run: |
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName="$EKS_CLUSTER" \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller

      - name: Show nodes & ingresses
        shell: bash
        run: |
          kubectl get nodes -o wide || true
          kubectl get ingress -A -o wide || true
